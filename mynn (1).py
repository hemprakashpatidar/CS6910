# -*- coding: utf-8 -*-
"""MyNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o7gyoQkVJyVTtjihBPwbWu0csrL5kjBU
"""



import math

import numpy as np
import random
import tensorflow as tf
from keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

random.seed(3)



!pip install wandb

label_array=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
opimg=[]
oplabel=[]
arr=[0,1,2,3,4,5,6,7,8,9]
isPresent=[0,0,0,0,0,0,0,0,0,0]
count=10
images=0
for i in range(100):
  if(isPresent[train_labels[i]]==0):
    isPresent[train_labels[i]]+=1
    count-=1
    opimg.append(train_images[i])
    oplabel.append(train_labels[i])
  if(count==0):
    break

"""Question 1"""

plt.figure(figsize=(10,10))
for i in range(10):
    plt.subplot(5,5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(opimg[i],cmap=plt.cm.binary_r)
    plt.xlabel(label_array[oplabel[i]])

"""Question 2"""

sweep_config = {
    'method': 'random', #grid, random
    'metric': {
      'name': 'accuracy',
        
    },
    'parameters': {
        'epochs': {
            'values': [2,3]
        },
        
        'learning_rate': {
            'values': [1e-2]
        },
    }
}

def sigmoid(x):
  return 1/(1+np.exp(-np.array(x)))

def der_sigmoid(X):
  d=1/(1 + np.exp(-np.array(X)))
  return d*(1-d)

def tanh(x):
  return (np.exp(np.array(x))-np.exp(-np.array(x)))/(np.exp(np.array(x))+np.exp(-np.array(x)))

def der_tanh(x):
  d=(np.exp(np.array(x))-np.exp(-np.array(x)))/(np.exp(np.array(x))+np.exp(-np.array(x)))
  return 1-d*d

def ReLU(x):
  return np.maximum(0,x)

def der_ReLU(x):
  x[x<0]=0
  x[x>0]=1
  return x

Hidden_layer=3
Neuron=[128,64,32,10]
outputs=10



import wandb

#wandb.init(project='My Assign', entity='hemprakashpatidar')
sweep_id = wandb.sweep(sweep_config, entity='hemprakashpatidar', project="My Assign")

class MyDNN(object):
  def __init__(self,Hidden_Layer,Neuron,eta,beta1,beta2,epoch,batch_size,initialisation,activfun,outfun,optimiser):
    self.Hidden_Layer=Hidden_Layer
    self.Neuron=Neuron
    self.eta=eta
    self.beta1=beta1
    self.beta2=beta2
    self.epoch=epoch
    self.batch_size=batch_size
    self.initialisation=initialisation
    self.activfun=activfun
    self.outfun=outfun
    self.optimiser=optimiser

  # Initilising parameters that will be used further  
  weight={}
  bias={}
  vdw={}
  vdb={}
  mw={}
  mb={}
  mwhat={}
  mbhat={}
  vwhat={}
  vbhat={}

  def optionals(self):
    self.vdw['0']= np.zeros((self.Neuron[0],784))
    self.vdb['0']= np.zeros((self.Neuron[0],1))
    self.mw['0']= np.zeros((self.Neuron[0],784))
    self.mb['0']= np.zeros((self.Neuron[0],1))
   
    for i in range(1,Hidden_layer + 1):
      self.vdw[str(i)]=np.zeros((self.Neuron[i],self.Neuron[i-1]))
      self.vdb[str(i)]=np.zeros((self.Neuron[i],1))
      self.mw[str(i)]=np.zeros((self.Neuron[i],self.Neuron[i-1]))
      self.mb[str(i)]=np.zeros((self.Neuron[i],1))

  #Initialisng weights and Biases
  def initWB(self,Hidden_layer,Neuron,initialisation):
    random.seed(3)
    if(initialisation=='random'):
      weight={'0':np.random.randn(Neuron[0],784)}
      for i in range(1,Hidden_layer):
        weight[str(i)]=np.random.randn(Neuron[i],Neuron[i-1])
      weight[str(Hidden_layer)]=np.random.randn(Neuron[Hidden_layer], Neuron[Hidden_layer-1])
      
      bias={}
      for i in range(0,Hidden_layer):
        bias[str(i)]=np.random.randn(Neuron[i],1)
      bias[str(Hidden_layer)]=np.random.randn(Neuron[Hidden_layer],1)

    if(initialisation=='Xavier'):
      weight={'0':np.random.normal(0,2.0/(Neuron[0]+784),(Neuron[0], 784))}
      for i in range(1,Hidden_layer):
        weight[str(i)]=np.random.normal(0,2.0/(Neuron[i]+Neuron[i-1]),(Neuron[i],Neuron[i-1]))
      weight[str(Hidden_layer)]=np.random.normal(0,2.0/(Neuron[Hidden_layer]+Neuron[Hidden_layer-1]),(Neuron[Hidden_layer],Neuron[Hidden_layer-1]))
     
      bias={}
      for i in range(0,Hidden_layer):
        bias[str(i)]=np.random.normal(0,2.0/(Neuron[i]+1),(Neuron[i],1))
      bias[str(Hidden_layer)]=np.random.normal(0,2.0/(Neuron[Hidden_layer]+1),(Neuron[Hidden_layer],1))
    return weight,bias

  #Preprocessing train data
  X_train=train_images.reshape(60000,784,-1)
  X_train=X_train/255
  Y_train={}
  for i in range(60000):
    temp=[]
    for j in range(10):
      if(train_labels[i]==j):
        temp.append(1.0)
      else:
        temp.append(0)
    Y_train[str(i)]=np.array(temp)
    Y_train[str(i)]=Y_train[str(i)].reshape(10,1)

  #Preprocessing test data
  X_test=test_images.reshape(10000,784,-1)
  X_test=X_test/255
  Y_test={}
  for i in range(10000):
    temp=[]
    for j in range(10):
      if(test_labels[i]==j):
        temp.append(1.0)
      else:
        temp.append(0)
    Y_test[str(i)]=np.array(temp)
    Y_test[str(i)]=Y_test[str(i)].reshape(10,1)

  #Forword propagation
  def feedforword(self,X_train,ip,weight,bias):
    pre_activation={}
    activation={}
    pre_activation['0']=np.dot(weight['0'],X_train[ip]) + bias['0']
    if(self.activfun=='tanh'):
      activation['0']=tanh(pre_activation['0'])
    elif(self.activfun=='sigmoid'):
      activation['0']=sigmoid(pre_activation['0'])
    elif(self.activfun=='ReLU'):
      activation['0']=ReLU(pre_activation['0'])
    for i in range(1,Hidden_layer):
      pre_activation[str(i)]=np.dot(weight[str(i)],activation[str(i-1)])+bias[str(i)]
      if(self.activfun=='tanh'):
        activation[str(i)]=tanh(pre_activation[str(i)])
      elif(self.activfun=='sigmoid'):
        activation[str(i)]=sigmoid(pre_activation[str(i)])
      elif(self.activfun=='ReLU'):
        activation[str(i)]=ReLU(pre_activation[str(i)])
    
    pre_activation[str(Hidden_layer)]=np.dot(weight[str(Hidden_layer)],activation[str(Hidden_layer-1)])+bias[str(Hidden_layer)]
    op={}
    den=np.sum(np.exp(pre_activation[str(Hidden_layer)])) 
    op['0']=np.exp(pre_activation[str(Hidden_layer)])/den
    return op,pre_activation,activation
  
  y={}
  preact={}
  act={}

  #backpropagation
  def backpropagation(self,op,y,Hidden_layer,weight,act,pre_act,ip):
    gradA={}
    gradH={}
    gradw={}
    gradb={}
    gradA[str(Hidden_layer)]=np.subtract(op['0'],y[str(ip)])
    gradw[str(Hidden_layer)]=np.dot(gradA[str(Hidden_layer)],np.transpose(act[str(Hidden_layer-1)]))/10
    gradb[str(Hidden_layer)]=gradA[str(Hidden_layer)]
    for i in reversed(range(Hidden_layer)):
      gradH[str(i)]=np.dot(np.transpose(weight[str(i+1)]),gradA[str(i+1)])
      if(self.activfun=='tanh'):
        gradA[str(i)]=[a*b for a,b in zip(gradH[str(i)],der_tanh(pre_act[str(i)]))]
      elif(self.activfun=='sigmoid'):
        gradA[str(i)]=[a*b for a,b in zip(gradH[str(i)],der_sigmoid(pre_act[str(i)]))]
      elif(self.activfun=='ReLU'):
        gradA[str(i)]=[a*b for a,b in zip(gradH[str(i)],der_ReLU(pre_act[str(i)]))]
      
      if i==0:
        gradw[str(i)]=np.dot(gradA['0'],np.transpose(self.X_train[ip]))
        gradb[str(i)]=gradA[str(i)]   
      else:
        gradw[str(i)]=np.dot(gradA[str(i)],np.transpose(act[str(i-1)]))
        gradb[str(i)]=gradA[str(i)] 
      
  
    return gradw,gradb


  def update(self,Hidden_layer,weight,bias,gradw,gradb,eta,it):
    if(self.optimiser=='sgd'):
      for i in range(Hidden_layer+1):
        weight[str(i)]=np.subtract(weight[str(i)],eta*(gradw[str(i)])/784)
        bias[str(i)]=np.subtract(bias[str(i)],eta*(gradb[str(i)])/784)
    
    elif(self.optimiser=='sgdmom'):
    
      for i in range(Hidden_layer+1):
        self.vdw[str(i)]=np.add(self.beta1*self.vdw[str(i)],eta*gradw[str(i)]/784)
        self.vdb[str(i)]=np.add(self.beta1*(self.vdb[str(i)]),eta*gradb[str(i)]/784)
        weight[str(i)]=np.subtract(weight[str(i)],(self.vdw[str(i)]))
        bias[str(i)]=np.subtract(bias[str(i)],(self.vdb[str(i)]))
    
    elif(self.optimiser=='adam'):
      for i in range(Hidden_layer+1):
        self.mw[str(i)]=self.beta1*self.mw[str(i)]+(1-self.beta1)*gradw[str(i)]
        self.mb[str(i)]=self.beta1*self.mb[str(i)]+(1-self.beta1)*gradb[str(i)]

        self.vdw[str(i)]=self.beta2*self.vdw[str(i)]+(1-self.beta2)*np.square(gradw[str(i)])
        self.vdb[str(i)]=self.beta2*self.vdb[str(i)]+(1-self.beta2)*np.square(gradb[str(i)])

        self.mwhat[str(i)]=self.mw[str(i)]/(1-math.pow(self.beta1,it+1))
        self.mbhat[str(i)]=self.mb[str(i)]/(1-math.pow(self.beta1,it+1))

        self.vwhat[str(i)]=self.vdw[str(i)]/(1-math.pow(self.beta2,it+1))
        self.vbhat[str(i)]=self.vdb[str(i)]/(1-math.pow(self.beta2,it+1))
       
        weight[str(i)]=np.subtract(weight[str(i)], (eta*0.1/np.sqrt(self.vwhat[str(i)]+10**-8))*self.mwhat[str(i)])
        bias[str(i)]=np.subtract(bias[str(i)], (eta*0.1/np.sqrt(self.vbhat[str(i)]+10**-8))*self.mbhat[str(i)])
    
    elif(self.optimiser=='rmsprop'):
      for i in range(Hidden_layer+1):
        self.vdw[str(i)]=self.beta2*self.vdw[str(i)]+(1-self.beta2)*np.square(gradw[str(i)])
        self.vdb[str(i)]=self.beta2*self.vdb[str(i)]+(1-self.beta2)*np.square(gradb[str(i)])

        self.vwhat[str(i)]=self.vdw[str(i)]/(1-math.pow(self.beta2,it+1))
        self.vbhat[str(i)]=self.vdb[str(i)]/(1-math.pow(self.beta2,it+1))
        
        weight[str(i)]=np.subtract(weight[str(i)], (eta*0.1/np.sqrt(self.vwhat[str(i)]+10**-8))*gradw[str(i)])
        bias[str(i)]=np.subtract(bias[str(i)], (eta*0.1/np.sqrt(self.vbhat[str(i)]+10**-8))*gradb[str(i)])
        
    


    return weight,bias
  


  def train(self):
    
    #wandb.init(config=config_defaults)
    #config = wandb.config
    #initWB()
    self.weight,self.bias=self.initWB(self.Hidden_Layer,self.Neuron,self.initialisation)
    it=0
    for epoch in range(self.epoch):
      dw={}
      db={}
      for i in range(50000):
        self.optionals()
        y,act,preact=self.feedforword(self.X_train,i,self.weight,self.bias)
        fw,fb=self.backpropagation(y,self.Y_train,self.Hidden_Layer,self.weight,act,preact,i)
        if(i%self.batch_size==0):
          for j in range(Hidden_layer+1):
            dw[str(j)]=fw[str(j)]
            db[str(j)]=fb[str(j)]
        else:
          for j in range(Hidden_layer+1):
            dw[str(j)]=np.add(dw[str(j)],fw[str(j)])
            db[str(j)]=np.add(db[str(j)],fb[str(j)])
        if(i%self.batch_size==self.batch_size-1):
          self.weight,self.bias=self.update(self.Hidden_Layer,self.weight,self.bias,dw,db,self.eta,it)
          it=it+1
       
      val_acc=self.ValAccuracy()
      print("Val_accuracy:",val_acc,"%")
      wandb.log({"Val accuracy": val_acc})
      
      train_acc=self.TrainAccuracy()
      print("Train_accuracy:",train_acc,"%")
      wandb.log({"Train accuracy": val_acc})
      
      acc=self.Accuracy()
      print("accuracy:",acc,"%")
      wandb.log({"accuracy": acc})

      valloss=self.Val_loss()      
      print("Val_Loss:",valloss,"%")
      wandb.log({"Val Loss": valloss})
      
      trainloss=self.train_loss()      
      print("Train_Loss:",trainloss,"%")
      wandb.log({"Train Loss": trainloss})
      
      loss=self.loss()      
      print("Loss:",loss,"%")
      wandb.log({"Loss": loss})
      
      
      wandb.log({"Epoch":epoch})
  
  
  
  def train_loss(self):
    ls=0
    for ip in range(50000):
      y,act,preact=self.feedforword(self.X_train,ip,self.weight,self.bias)
      ls+=np.sum(-self.Y_train[str(ip)]*np.log(y['0']))
    return ls/10000


  def Val_loss(self):
    ls=0
    for ip in range(50000,60000):
      y,act,preact=self.feedforword(self.X_train,ip,self.weight,self.bias)
      ls+=np.sum(-self.Y_train[str(ip)]*np.log(y['0']))
    return ls/10000

  def loss(self):
    ls=0
    for ip in range(10000):
      y,act,preact=self.feedforword(self.X_test,ip,self.weight,self.bias)
      ls+=np.sum(-self.Y_test[str(ip)]*np.log(y['0']))
    return ls/10000

  def ValAccuracy(self):
    s=0
    for i in range(50000,60000):
      pred=[]
      for j in range(10):
        pred.append(0)
      z,zz,zzz=self.feedforword(self.X_train,i,self.weight,self.bias)
      pred[np.argmax(z['0'])]=1
      s+=np.dot(pred,self.Y_train[str(i)])[0]
    return s/10000
  
  def TrainAccuracy(self):
    s=0
    for i in range(50000):
      pred=[]
      for j in range(10):
        pred.append(0)
      z,zz,zzz=self.feedforword(self.X_train,i,self.weight,self.bias)
      pred[np.argmax(z['0'])]=1
      s+=np.dot(pred,self.Y_train[str(i)])[0]
    return s/10000
  def Accuracy(self):
    s=0
    for i in range(10000):
      pred=[]
      for j in range(10):
        pred.append(0)
      z,zz,zzz=self.feedforword(self.X_test,i,self.weight,self.bias)
      pred[np.argmax(z['0'])]=1
      s+=np.dot(pred,self.Y_test[str(i)])[0]
    return s/10000

sweep_config = {
    'method': 'random', #grid, random
    'metric': {
      'name': 'accuracy',
        
    },
    'parameters': {
        'epochs': {
            'values': [2,3]
        },
        
        'learning_rate': {
            'values': [1e-3]
        },

        'optimiser': {
            'values': ['sgd','sgdmom']
        },
    }
}

wandb.init()

wandb.sweep(sweep_config)
'''config_defaults = {
            'epochs': 5,
            'learning_rate': 1e-2
            
          }

'''

def train_m():
  run = wandb.init()
  config=run.config
  model = MyDNN(3,[128,128,128,10],run.config.learning_rate,0.9,0.999,config.epochs,64,'random','tanh','softmax', config.optimiser)
          
  model.train()
    
  

wandb.agent(sweep_id,train_m)

selfd=MyDNN(3,[64,64,64,10],0.001,0.9,0.999,1,16,'random','tanh','softmax','adam')

selfd.train()